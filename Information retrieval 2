import os
import math
from collections import defaultdict
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import nltk

# Ensure NLTK resources are available
nltk.download('stopwords')
nltk.download('punkt')

# Initialize components
ps = PorterStemmer()
stop_words = set(stopwords.words('english'))

# Directory of .txt files
CORPUS_DIR = '/content'  # Replace with the actual path to your "corpus" folder

# Step 1: Preprocessing
def preprocess(text):
    # Tokenize and remove punctuation
    tokens = word_tokenize(text)
    tokens = [word.lower() for word in tokens if word.isalnum()]
    
    # Remove stop words and apply stemming
    tokens = [ps.stem(word) for word in tokens if word not in stop_words]
    return tokens

def read_corpus(corpus_dir):
    docs = {}
    for idx, filename in enumerate(os.listdir(corpus_dir)):
        if filename.endswith('.txt'):
            with open(os.path.join(corpus_dir, filename), 'r', encoding='utf-8') as f:
                docs[idx] = preprocess(f.read())  # docID is idx
    return docs

# Step 2: Build Dictionary and Postings List
def build_index(documents):
    dictionary = defaultdict(list)
    for doc_id, tokens in documents.items():
        term_freqs = defaultdict(int)
        for token in tokens:
            term_freqs[token] += 1

        # Add term frequencies to postings list
        for term, freq in term_freqs.items():
            dictionary[term].append((doc_id, freq))
    return dictionary

# Step 3: Compute tf weights for documents (lnc) and idf values
def compute_tf_weights_and_idf(dictionary, N):
    tf_weights = defaultdict(dict)
    idf_values = {}
    
    # Compute idf values (log base 10)
    for term, postings in dictionary.items():
        df = len(postings)
        idf_values[term] = math.log10(N / df)

    # Compute tf weights for documents (lnc)
    doc_lengths = {}
    for doc_id in range(N):
        doc_vector = {}
        for term, postings in dictionary.items():
            for posting in postings:
                if posting[0] == doc_id:
                    freq = posting[1]
                    tf = 1 + math.log10(freq)  # Log base 10 for term frequency
                    doc_vector[term] = tf  # Do not multiply by idf
        # Compute document length for normalization
        doc_lengths[doc_id] = math.sqrt(sum(weight**2 for weight in doc_vector.values()))
        tf_weights[doc_id] = doc_vector

    return tf_weights, idf_values, doc_lengths

# Step 4: Process Query Using ltc (include idf)
def process_query(query, idf_values):
    query_tokens = preprocess(query)
    term_freqs = defaultdict(int)
    
    for token in query_tokens:
        term_freqs[token] += 1
    
    query_vector = {}
    for term, freq in term_freqs.items():
        if term in idf_values:
            tf = 1 + math.log10(freq)  # Log base 10 for term frequency
            idf = idf_values[term]  # Log base 10 for idf
            query_vector[term] = tf * idf  # Multiply by idf for queries
    return query_vector

# Step 5: Cosine Similarity for Queries
def cosine_similarity(query_vector, doc_vector, doc_magnitude, query_magnitude):
    dot_product = sum(query_vector.get(term, 0) * doc_vector.get(term, 0) for term in query_vector)
    if query_magnitude * doc_magnitude == 0:
        return 0
    return dot_product / (query_magnitude * doc_magnitude)

# Step 6: Rank Documents by Cosine Similarity
def rank_documents(query, tf_weights, doc_lengths, idf_values):
    query_vector = process_query(query, idf_values)
    query_magnitude = math.sqrt(sum(weight**2 for weight in query_vector.values()))
    doc_scores = []
    
    for doc_id, doc_vector in tf_weights.items():
        doc_magnitude = doc_lengths[doc_id]
        score = cosine_similarity(query_vector, doc_vector, doc_magnitude, query_magnitude)
        doc_scores.append((doc_id, score))
    
    # Sort by score in descending order, then by doc_id
    ranked_docs = sorted(doc_scores, key=lambda x: (-x[1], x[0]))
    return ranked_docs[:10]

# Full flow: Read corpus, build index, and query

# Read corpus and build index
documents = read_corpus(CORPUS_DIR)
N = len(documents)  # Total number of documents
dictionary = build_index(documents)
tf_weights, idf_values, doc_lengths = compute_tf_weights_and_idf(dictionary, N)

# Query and retrieve top 10 documents
query = "Developing your Zomato business account and profile is a great way to boost your restaurantâ€™s online reputation"  # Replace with user input
ranked_docs = rank_documents(query, tf_weights, doc_lengths, idf_values)

# Output top 10 documents
for doc_id, score in ranked_docs:
    print(f"Document ID: {doc_id}, Score: {score}")
